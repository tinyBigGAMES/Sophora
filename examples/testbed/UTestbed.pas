{===============================================================================
  ___           _
 / __| ___ _ __| |_  ___ _ _ __ _ ™
 \__ \/ _ \ '_ \ ' \/ _ \ '_/ _` |
 |___/\___/ .__/_||_\___/_| \__,_|
          |_|
 AI Reasoning, Function-calling &
       Knowledge Retrieval

 Copyright © 2025-present tinyBigGAMES™ LLC
 All Rights Reserved.

 https://github.com/tinyBigGAMES/Sophora

 See LICENSE file for license information
===============================================================================}

(*
  === USAGE NOTES ===
  1. Download model from:
     - https://huggingface.co/tinybiggames/DeepHermes-3-Llama-3-8B-Preview-Q4_K_M-GGUF/resolve/main/deephermes-3-llama-3-8b-preview-q4_k_m.gguf?download=true
     - https://huggingface.co/tinybiggames/bge-m3-Q8_0-GGUF/resolve/main/bge-m3-q8_0.gguf?download=true
  2. Place in your desired location, the examples expect:
     - C:/LLM/GGUF

  3. Get search api key from:
     - https://tavily.com/
     - You get 1000 free tokens per month
     - Create a an environment variable named "TAVILY_API_KEY" and set it to
       the search api key.
*)

unit UTestbed;

interface

uses
  System.SysUtils,
  Sophora.CLibs,
  Sophora.Utils,
  Sophora.Common,
  Sophora.Messages,
  Sophora.Inference,
  Sophora.RAG,
  Sophora.Console;

procedure RunTests();

implementation


{
  This example demonstrates how to utilize DeepHermesLarge Language Model (LLM)
  in non-thinking mode to achieve the fastest possible response time. The
  function initializes a message queue and inference engine, loads the model,
  and processes a user query with immediate token streaming. The response is
  displayed in real-time as tokens are generated.

  Additionally, performance metrics, including input tokens, output tokens,
  and processing speed, are printed at the end of execution.
}
procedure Test01();
var
  // Stores the messages exchanged between user and LLM
  LMsg: TsoMessages;

  // Represents the inference engine responsible for running the LLM
  LInf: TsoInference;

  // Holds the count of input tokens processed by the model
  LInputTokens: Integer;

  // Holds the count of output tokens generated by the model
  LOutputTokens: Integer;

  // Represents the speed of token generation (tokens per second)
  LTokenSpeed: Single;
begin
  // Create instances of message handler and inference engine
  LMsg := TsoMessages.Create();
  LInf := TsoInference.Create();

  try
    // Load the LLM model; exit if loading fails
    if not LInf.LoadModel() then Exit;

    // Define an event to handle each token as it's generated
    LInf.NextTokenEvent :=
      procedure(const AToken: string)
      begin
        // Print generated tokens in green for real-time display
        soConsole.Print(soCSIFGGreen + AToken);
      end;

    // Add a user query to the message queue
    LMsg.Add(soUser, 'who is bill gates?');

    // Print the user question with formatting
    soConsole.PrintLn('Question: %s%s' + soCRLF, [soCSIFGCyan + soCRLF, LMsg.LastUser()]);

    // Print response header
    soConsole.PrintLn('Response:');

    // Execute inference with the provided user query
    if LInf.Run(LMsg) then
    begin
      // Retrieve and display performance metrics
      LInf.Performance(@LInputTokens, @LOutputTokens, @LTokenSpeed);
      soConsole.PrintLn(soCRLF + soCRLF + 'Performance:' + soCRLF +
        soCSIFGYellow + 'Input : %d tokens' + soCRLF +
        'Output: %d tokens' + soCRLF +
        'Speed : %3.2f tokens/sec',
        [LInputTokens, LOutputTokens, LTokenSpeed]);
    end
    else
    begin
      // Display error message in red if inference fails
      soConsole.PrintLn(soCRLF + soCRLF + soCSIFGRed + 'Errors: %s', [LInf.GetError()]);
    end;
  finally
    // Free allocated resources to avoid memory leaks
    if Assigned(LInf) then LInf.Free();
    if Assigned(LMsg) then LMsg.Free();
  end;
end;


{
  This example demonstrates how to utilize DeepHermes Large Language Model
  (LLM) in "thinking" mode. Unlike the non-thinking mode, this mode enables
  deep reasoning and internal deliberation. The model is instructed to
  carefully analyze the problem, engage in systematic reasoning, and provide
  detailed thoughts within <think></think> XML tags before delivering a final
  answer.

  The function initializes a message queue and inference engine, loads the
  model, and processes a user query using a structured reasoning approach. The
  response includes both the AI's internal thought process and the final
  answer. Performance metrics, including input tokens, output tokens, and
  processing speed, are displayed at the end of execution.
}
procedure Test02();
var
  // Stores the messages exchanged between user and LLM
  LMsg: TsoMessages;

  // Represents the inference engine responsible for running the LLM
  LInf: TsoInference;

  // Holds the count of input tokens processed by the model
  LInputTokens: Integer;

  // Holds the count of output tokens generated by the model
  LOutputTokens: Integer;

  // Represents the speed of token generation (tokens per second)
  LTokenSpeed: Single;
begin
  // Create instances of message handler and inference engine
  LMsg := TsoMessages.Create();
  LInf := TsoInference.Create();

  try
    // Load the LLM model; exit if loading fails
    if not LInf.LoadModel() then Exit;

    // Define an event to handle each token as it's generated
    LInf.NextTokenEvent :=
      procedure(const AToken: string)
      begin
        // Print generated tokens in green for real-time display
        soConsole.Print(soCSIFGGreen + AToken);
      end;

    // Provide a system instruction that enables deep reasoning mode
    LMsg.Add(soSystem, 'You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> XML tags, and then provide your solution or response to the problem. After your thinking process, clearly state your final answer or conclusion outside the XML tags.');

    // Add a complex user query to be analyzed in deep thinking mode
    LMsg.Add(soUser, 'I walk on four legs in the morning, two legs at noon, and three legs in the evening. But beware, for this is not the famous riddle of the Sphinx. Instead, my journey is cyclical, and each stage is both an end and a beginning. I am not a creature, but I hold the essence of all creatures within me. What am I?');

    // Print the user question with formatting
    soConsole.PrintLn('Question: %s%s' + soCRLF, [soCSIFGCyan + soCRLF, LMsg.LastUser()]);

    // Print response header
    soConsole.PrintLn('Response:');

    // Execute inference with the provided user query
    if LInf.Run(LMsg) then
    begin
      // Retrieve and display performance metrics
      LInf.Performance(@LInputTokens, @LOutputTokens, @LTokenSpeed);
      soConsole.PrintLn(soCRLF + soCRLF + 'Performance:' + soCRLF +
        soCSIFGYellow + 'Input : %d tokens' + soCRLF +
        'Output: %d tokens' + soCRLF +
        'Speed : %3.2f tokens/sec',
        [LInputTokens, LOutputTokens, LTokenSpeed]);
    end
    else
    begin
      // Display error message in red if inference fails
      soConsole.PrintLn(soCRLF + soCRLF + soCSIFGRed + 'Errors: %s', [LInf.GetError()]);
    end;
  finally
    // Free allocated resources to avoid memory leaks
    if Assigned(LInf) then LInf.Free();
    if Assigned(LMsg) then LMsg.Free();
  end;
end;


{
  This example demonstrates how to generate embeddings using a Large Language
  Model (LLM). Embeddings are numerical representations of textual data that
  can be used for tasks such as similarity search, clustering, and
  retrieval-augmented generation (RAG).

  This function initializes the embedding engine, loads the model, and
  processes a given prompt to generate its embedding vector. The resulting
  embeddings are printed to the console in a formatted manner.
}
procedure Test03();
var
  // Represents the embedding engine responsible for generating vector
  // representations
  LEmb: TsoEmbeddings;

  // Stores the resulting embedding vector (array of floating-point numbers)
  LResult: TArray<Single>;

  // Iterator for looping through embedding values
  I, LLen: Integer;

  // Holds the text prompt to be converted into an embedding vector
  LPrompt: string;

  // Formatting variable for comma separation in output
  LComma: string;
begin
  // Create an instance of the embedding engine
  LEmb := TsoEmbeddings.Create();

  try
    // Load the embedding model; exit if loading fails
    if not LEmb.LoadModel() then Exit;

    // Define the text prompt to be embedded
    LPrompt := 'Explain how data analysis supports machine learning.';

    // Print the input prompt with formatting
    soConsole.PrintLn('Prompt: %s%s' + soCRLF, [soCSIFGCyan + soCRLF, LPrompt]);

    // Generate the embedding vector from the given prompt
    LResult := LEmb.Generate(LPrompt);

    // Print the header for the embedding values
    soConsole.PrintLn('Embeddings:');

    // Get the length of the resulting embedding vector
    LLen := High(LResult) + 1;

    // Loop through the embedding vector and print values
    for I := 0 to LLen - 1 do
    begin
      if I <> LLen - 1 then
        LComma := ','
      else
        LComma := '';

      // Print each embedding value in green, formatted to six decimal places
      soConsole.Print(soCSIFGGreen + '%0.6f%s ', [LResult[I], LComma]);
    end;

    // Print a newline for formatting
    soConsole.PrintLn();

  finally
    // Free allocated resources to avoid memory leaks
    LEmb.Free();
  end;
end;


{
  This procedure serves as a test harness for running different test cases
  related to the Large Language Model (LLM) functionalities, such as
  non-thinking mode, deep-thinking mode, and embedding generation.

  The selected test is executed based on a predefined integer value, and the
  console output is formatted to clearly indicate which test is running.
}
procedure RunTests();
var
  // Holds the test number to execute
  LNum: Integer;
begin
  // Print the Sophora version in magenta for visibility
  soConsole.PrintLn(soCSIFGMagenta + 'Sophora v%s' + soCRLF, [CsoSophoraVersion]);

  // Set the test number to execute
  LNum := 01;

  // Execute the corresponding test based on the selected test number
  case LNum of
    01: Test01();  // Runs the non-thinking mode test
    02: Test02();  // Runs the deep-thinking mode test
    03: Test03();  // Runs the embedding generation test
  end;

  // Pause execution to allow viewing the console output before exiting
  soConsole.Pause();
end;


end.
